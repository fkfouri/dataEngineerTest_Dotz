{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codigos para manipulação do Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name):\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    bucket.storage_class = \"STANDARD\"\n",
    "    new_bucket = storage_client.create_bucket(bucket, location=\"us\")\n",
    "\n",
    "    print(\n",
    "        \"Created bucket {} in {} with storage class {}\".format(\n",
    "            new_bucket.name, new_bucket.location, new_bucket.storage_class\n",
    "        )\n",
    "    )\n",
    "    return new_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_bucket(bucket_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    \n",
    "    for i in storage_client.list_blobs(bucket_name): \n",
    "        i.delete()\n",
    "    \n",
    "    bucket.delete()\n",
    "\n",
    "    print(\"Bucket {} deleted\".format(bucket.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, destination_blob_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codigos de Manipulação BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_BQ(dataset_id):\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Construct a full Dataset object to send to the API.\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "    # TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "    dataset.location = \"US\"\n",
    "\n",
    "    dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_dataset_BD(dataset_id):\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    client.delete_dataset(\n",
    "        dataset_id, delete_contents=True, not_found_ok=True\n",
    "    )  # Make an API request.\n",
    "\n",
    "    print(\"Deleted dataset '{}'.\".format(dataset_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_table(table_id):\n",
    "    client = bigquery.Client()\n",
    "    try:\n",
    "        job = client.delete_table( table_id )\n",
    "        job.result()\n",
    "        print(f'{ table_id } was removed.')\n",
    "    except:\n",
    "        print(f'The { table_id } is not exist.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcões Apache BEAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadGcsBlobs(beam.DoFn):\n",
    "    def process(self, element, *args, **kwargs):\n",
    "        from apache_beam.io.gcp import gcsio\n",
    "        gcs = gcsio.GcsIO()\n",
    "        yield (element, gcs.open(element).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(readable_file):\n",
    "    import io\n",
    "    import numpy as np\n",
    "    \n",
    "    buffer = io.StringIO(readable_file[1].decode())\n",
    "    df = pd.read_csv(filepath_or_buffer = buffer, header = 0)\n",
    "\n",
    "    yield df.replace({np.nan:None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_boss_comp(df):       \n",
    "    df.unique_feature = df.unique_feature.map(dict(Yes=1, No=0))\n",
    "    df.groove = df.groove.map(dict(Yes=1, No=0))\n",
    "    df.orientation = df.orientation.map(dict(Yes=1, No=0))\n",
    "    \n",
    "    #.to_dict('records') \n",
    "    #.values.tolist() \n",
    "    #.T.to_dict().values()\n",
    "    #qs = df.replace({np.nan:None}).to_dict('records')\n",
    "    #l= [v for k,v in qs.items()]\n",
    "\n",
    "    temp = df.to_dict('records')\n",
    "    yield temp[0]  #funcionou\n",
    "    \n",
    "    #print({'items': temp})\n",
    "    #yield df.replace({np.nan:None}).T.to_dict().values()\n",
    "    #yield {'items': temp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_price_quote(df):\n",
    "    df.bracket_pricing = df.bracket_pricing.map(dict(Yes=1, No=0))\n",
    "    \n",
    "    temp = df.to_dict('records')\n",
    "    \n",
    "    yield temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bill_of_materials(df):\n",
    "    df = df[['tube_assembly_id']]\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    temp = df.to_dict('records')\n",
    "    \n",
    "    yield temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bill_of_materials_to_product_structure(df):\n",
    "    df8 = df[df.component_id_8.notnull()][['tube_assembly_id', 'component_id_8','quantity_8']].rename(columns={'component_id_8': 'component_id', 'quantity_8' : 'quantity'})\n",
    "    df7 = df[df.component_id_7.notnull()][['tube_assembly_id', 'component_id_7','quantity_7']].rename(columns={'component_id_7': 'component_id', 'quantity_7' : 'quantity'})\n",
    "    df6 = df[df.component_id_6.notnull()][['tube_assembly_id', 'component_id_6','quantity_6']].rename(columns={'component_id_6': 'component_id', 'quantity_6' : 'quantity'})\n",
    "    df5 = df[df.component_id_5.notnull()][['tube_assembly_id', 'component_id_5','quantity_5']].rename(columns={'component_id_5': 'component_id', 'quantity_5' : 'quantity'})\n",
    "    df4 = df[df.component_id_4.notnull()][['tube_assembly_id', 'component_id_4','quantity_4']].rename(columns={'component_id_4': 'component_id', 'quantity_4' : 'quantity'})\n",
    "    df3 = df[df.component_id_3.notnull()][['tube_assembly_id', 'component_id_3','quantity_3']].rename(columns={'component_id_3': 'component_id', 'quantity_3' : 'quantity'})\n",
    "    df2 = df[df.component_id_2.notnull()][['tube_assembly_id', 'component_id_2','quantity_2']].rename(columns={'component_id_2': 'component_id', 'quantity_2' : 'quantity'})\n",
    "    df1 = df[df.component_id_1.notnull()][['tube_assembly_id', 'component_id_1','quantity_1']].rename(columns={'component_id_1': 'component_id', 'quantity_1' : 'quantity'})\n",
    "    \n",
    "    \n",
    "    df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8], ignore_index=True)\n",
    "  \n",
    "    temp = df.to_dict('records')\n",
    "    \n",
    "    yield temp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dados do projeto GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"fk_dotz_bucket\"\n",
    "\n",
    "PROJECT='teste-dotz-292803' \n",
    "dataset = 'dotz'\n",
    "REGION='us-east1-b'\n",
    "\n",
    "options = {\n",
    "      'project': PROJECT,\n",
    "      'region': REGION,\n",
    "      'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "      'no_save_main_session': True,\n",
    "  }\n",
    "    \n",
    "RUNNER = 'DirectRunner' #DataflowRunner \n",
    "opts = beam.pipeline.PipelineOptions(flags = [], **options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do Bucket e Upload de arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket fk_dotz_bucket in US with storage class STANDARD\n",
      "File E:\\Projetos\\Jobs\\DataEngineer\\Dotz/dataset/bill_of_materials.csv uploaded to dataset/bill_of_materials.csv.\n",
      "File E:\\Projetos\\Jobs\\DataEngineer\\Dotz/dataset/comp_boss.csv uploaded to dataset/comp_boss.csv.\n",
      "File E:\\Projetos\\Jobs\\DataEngineer\\Dotz/dataset/price_quote.csv uploaded to dataset/price_quote.csv.\n"
     ]
    }
   ],
   "source": [
    "create_bucket(bucket_name)\n",
    "upload_to_bucket(bucket_name, f'{ os.getcwd() }/dataset/bill_of_materials.csv', 'dataset/bill_of_materials.csv')\n",
    "upload_to_bucket(bucket_name, f'{ os.getcwd() }/dataset/comp_boss.csv', 'dataset/comp_boss.csv')\n",
    "upload_to_bucket(bucket_name, f'{ os.getcwd() }/dataset/price_quote.csv', 'dataset/price_quote.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de um DataSet no BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset teste-dotz-292803.dotz\n"
     ]
    }
   ],
   "source": [
    "create_dataset_BQ(f'{PROJECT}.{dataset}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIPELINE Apache Beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(origin, table, schema, _function):\n",
    "    with beam.Pipeline(RUNNER, options = opts) as p:\n",
    "        files = (p\n",
    "                 | \"Initialize\" >> beam.Create([f\"gs://{bucket_name}/dataset/{origin}\"])\n",
    "                 | \"Read blobs\" >> beam.ParDo(ReadGcsBlobs())\n",
    "                 | \"Convert to Pandas \" >> beam.FlatMap(create_dataframe)\n",
    "                 | \"convert function \" >> beam.FlatMap(_function)\n",
    "                 | f'create table {table}' >> beam.io.gcp.bigquery.WriteToBigQuery(\n",
    "                                    table=f'teste-dotz-292803:dotz.{ table }',\n",
    "                                    schema=schema ,\n",
    "                                    method=\"STREAMING_INSERTS\",\n",
    "                                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "                                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED\n",
    "                                ) \n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        var import_html = () => {\n",
       "          ['https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html'].forEach(href => {\n",
       "            var link = document.createElement('link');\n",
       "            link.rel = 'import'\n",
       "            link.href = href;\n",
       "            document.head.appendChild(link);\n",
       "          });\n",
       "        }\n",
       "        if ('import' in document.createElement('link')) {\n",
       "          import_html();\n",
       "        } else {\n",
       "          var webcomponentScript = document.createElement('script');\n",
       "          webcomponentScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js';\n",
       "          webcomponentScript.type = 'text/javascript';\n",
       "          webcomponentScript.onload = function(){\n",
       "            import_html();\n",
       "          };\n",
       "          document.head.appendChild(webcomponentScript);\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\projetos\\jobs\\dataengineer\\dotz\\env\\lib\\site-packages\\apache_beam\\io\\gcp\\bigquery.py:1574: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  experiments = p.options.view_as(DebugOptions).experiments or []\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline('comp_boss.csv',\n",
    "             'component',\n",
    "             'component_id:STRING, component_type_id:STRING, type:STRING, connection_type_id:STRING, outside_shape:STRING, base_type:STRING, height_over_tube:FLOAT, bolt_pattern_long:FLOAT, bolt_pattern_wide:FLOAT,' + \\\n",
    "                'groove:INTEGER, base_diameter:FLOAT, shoulder_diameter:FLOAT, unique_feature:INTEGER, orientation:INTEGER, weight:FLOAT',\n",
    "             convert_boss_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline('price_quote.csv',\n",
    "             'quotation',\n",
    "             'tube_assembly_id:STRING, supplier:STRING, quote_date:STRING, annual_usage:INTEGER, min_order_quantity:INTEGER, bracket_pricing:INTEGER, quantity:INTEGER, cost:FLOAT',\n",
    "             convert_boss_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline('bill_of_materials.csv',\n",
    "             'product_structure',\n",
    "             'tube_assembly_id:STRING, component_id:STRING, quantity:INTEGER',\n",
    "             convert_bill_of_materials_to_product_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.gcp.bigquery_tools:Sleeping for 150 seconds before the write as BigQuery inserts can be routed to deleted table for 2 mins after the delete and create.\n"
     ]
    }
   ],
   "source": [
    "run_pipeline('bill_of_materials.csv',\n",
    "             'tube_assembly',\n",
    "             'idx:INTEGER, tube_assembly_id:STRING',\n",
    "            convert_bill_of_materials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção das tabelas e dataset BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The teste-dotz-292803.dotz.component is not exist.\n",
      "The teste-dotz-292803.dotz.quotation is not exist.\n",
      "The teste-dotz-292803.dotz.tube_assembly is not exist.\n",
      "The teste-dotz-292803.dotz.product_structure is not exist.\n",
      "Deleted dataset 'dotz'.\n"
     ]
    }
   ],
   "source": [
    "delete_table(f'{ PROJECT }.{ dataset }.component')\n",
    "delete_table(f'{ PROJECT }.{ dataset }.quotation')\n",
    "delete_table(f'{ PROJECT }.{ dataset }.tube_assembly')\n",
    "delete_table(f'{ PROJECT }.{ dataset }.product_structure')\n",
    "delete_dataset_BD(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção do Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket fk_dotz_bucket deleted\n"
     ]
    }
   ],
   "source": [
    "delete_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
